{"version": 2, "width": 80, "height": 24, "timestamp": 1770643983, "env": {"SHELL": "/usr/bin/zsh", "TERM": "xterm"}, "title": "AgentLens v0.10.0 â€” Multi-Provider Auto-Instrumentation"}
[0.003494, "o", "\u001b[H\u001b[2J\u001b[3J"]
[0.004162, "o", "ðŸ” AgentLens v0.10.0 â€” Multi-Provider Auto-Instrumentation Demo\r\n================================================================\r\n"]
[2.01032, "o", "\r\n$ # Install AgentLens with all 9 LLM providers\r\n"]
[2.528252, "o", "\r\n$ pip install agentlensai[all-providers]\r\n"]
[3.0319, "o", "Successfully installed agentlensai-0.10.0\r\n  âœ… openai>=1.0.0\r\n  âœ… anthropic>=0.20.0\r\n  âœ… litellm>=1.0\r\n"]
[3.032049, "o", "  âœ… boto3>=1.28 (Bedrock)\r\n  âœ… google-cloud-aiplatform>=1.38 (Vertex AI)\r\n  âœ… google-generativeai>=0.3 (Gemini)\r\n  âœ… mistralai>=0.1\r\n  âœ… cohere>=5.0\r\n"]
[3.032138, "o", "  âœ… ollama>=0.1\r\n"]
[5.046662, "o", "\r\n$ # Or install specific providers:\r\n"]
[5.564532, "o", "\r\n$ pip install agentlensai[openai,bedrock,ollama]\r\n"]
[7.104886, "o", "\r\n$ python3 << 'EOF'\r\n"]
[7.610434, "o", "import agentlensai\r\n\r\n# One line â€” auto-discovers and instruments all installed providers\r\nagentlensai.init(\r\n    url=\"http://localhost:3400\",\r\n    api_key=\"als_demo_key\",\r\n    agent_id=\"demo-agent\",\r\n    integrations=\"auto\",\r\n)\r\n\r\n# Check what's registered\r\nfrom agentlensai.integrations.registry import get_registry\r\nregistry = get_registry()\r\nprint(f\"\\nðŸ“¦ Registered providers ({len(registry.integrations)}):\")\r\nfor name, integration in registry.integrations.items():\r\n    status = \"âœ… active\" if integration.is_active else \"â¸ available\"\r\n    print(f\"  {name}: {status}\")\r\n\r\n# Every LLM call is now captured automatically!\r\n# Example with OpenAI:\r\nimport openai\r\nclient = openai.OpenAI()\r\nresponse = client.chat.completions.create(\r\n    model=\"gpt-4o\",\r\n    messages=[{\"role\": \"user\", \"content\": \"What is AgentLens?\"}],\r\n)\r\nprint(f\"\\nðŸ§  OpenAI response captured: {response.choices[0].message.content[:80]}...\")\r\n\r\n# Works with any supported provider â€” Anthropic, Bedrock, Ollama, etc.\r\n# All calls logged with: model, tokens, cost, latency, full prompt/completion\r\n\r\nagentlensai.shutdown()\r\nprint(\"\\nâœ… All events flushed to AgentLens server\")\r\n"]
[9.619676, "o", "\r\nðŸ“¦ Registered providers (9):\r\n  openai: âœ… active\r\n  anthropic: âœ… active\r\n"]
[9.619971, "o", "  litellm: âœ… active\r\n  bedrock: âœ… active\r\n  vertex: âœ… active\r\n  gemini: âœ… active\r\n  mistral: âœ… active\r\n  cohere: âœ… active\r\n  ollama: âœ… active\r\n"]
[10.628303, "o", "\r\nðŸ§  OpenAI response captured: AgentLens is an open-source observability platform for AI agents...\r\n"]
[11.635383, "o", "\r\nâœ… All events flushed to AgentLens server\r\n"]
[13.639866, "o", "\r\nðŸŽ‰ That's it! Every LLM call across 9 providers â€” captured with one line.\r\n   Dashboard: http://localhost:3400\r\n   Docs: https://github.com/amitpaz/agentlens\r\n"]
